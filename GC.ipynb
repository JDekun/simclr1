{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JDekun/simclr1/blob/master/GC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvYRyZ27v4NX",
        "outputId": "7c2031d5-b4cb-4365-be9b-f1a057b35d46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Kaggle'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 17 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/Kaggle/kaggle.json'\n",
            "Downloading cifar-10-batches-py.zip to cifar-10-batches-py\n",
            " 92% 150M/162M [00:05<00:00, 26.9MB/s]\n",
            "100% 162M/162M [00:05<00:00, 30.8MB/s]\n",
            "Cloning into 'simclr1'...\n",
            "remote: Enumerating objects: 145, done.\u001b[K\n",
            "remote: Counting objects: 100% (145/145), done.\u001b[K\n",
            "remote: Compressing objects: 100% (102/102), done.\u001b[K\n",
            "remote: Total 145 (delta 87), reused 90 (delta 41), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (145/145), 85.71 KiB | 196.00 KiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n",
            "/content/working\n"
          ]
        }
      ],
      "source": [
        "#@title ËØ∑ËæìÂÖ•Êï∞ÊçÆÈõÜÂêçÁß∞\n",
        "\n",
        "# ÂØºÂÖ•Kaggle API\n",
        "!git clone https://github.com/JDekun/Kaggle.git\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/Kaggle' #Ê≥®ÊÑèkaggleÊñá‰ª∂Â§πÂåÖÂê´jsonÊñá‰ª∂ \n",
        "\n",
        "!mkdir /content/input\n",
        "os.chdir('/content/input')\n",
        "\n",
        "# Êï∞ÊçÆÈõÜÂêçÂ≠ó\n",
        "datasets = [\"janzenliu/cifar-10-batches-py\"]  #@param {type:\"raw\"}\n",
        "len_mydekun = len(datasets)\n",
        "\n",
        "if len_mydekun != 0:\n",
        "  for i in range(len_mydekun):\n",
        "    # ‰∏ãËΩΩÊï∞ÊçÆÈõÜ\n",
        "    temp = datasets[i]\n",
        "    name, dataset = temp.split('/')\n",
        "    \n",
        "    zip = dataset + '.zip'\n",
        "    !kaggle datasets download -d $temp -p $dataset\n",
        "\n",
        "    # Ëß£ÂéãÊï∞ÊçÆÈõÜÂπ∂Âà†Èô§ÂéãÁº©ÂåÖ\n",
        "    !unzip $dataset/$zip -d $dataset > /dev/null 2>&1\n",
        "    !rm -f $dataset/$zip\n",
        "\n",
        "# ËÆæÁΩÆËøêË°åÊó∂ÁöÑÊ†πÁõÆÂΩï\n",
        "!mkdir /content/working\n",
        "os.chdir('/content/working')\n",
        "\n",
        "# GitHubÂêçÂ≠ó\n",
        "github = [\"https://github.com/JDekun/simclr1.git\" ]  #@param {type:\"raw\"}\n",
        "len_github = len(github)\n",
        "if len_github != 0:\n",
        "  for i in range(len_github):\n",
        "    temp = github[i]\n",
        "    !git clone $temp\n",
        "\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdeadOduWnjJ",
        "outputId": "896a3ff2-d9f1-468a-9238-38687de857ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 9994, done.\u001b[K\n",
            "remote: Counting objects: 100% (113/113), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 9994 (delta 46), reused 68 (delta 20), pack-reused 9881\u001b[K\n",
            "Receiving objects: 100% (9994/9994), 14.91 MiB | 9.06 MiB/s, done.\n",
            "Resolving deltas: 100% (6847/6847), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install thop wandb > /dev/null 2>&1\n",
        "!git clone https://github.com/NVIDIA/apex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acW-qtMsNBLD",
        "outputId": "57255a9f-b256-42d2-cd41-772f1513c8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/working/apex\n"
          ]
        }
      ],
      "source": [
        "cd apex/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eakPCSt_NEFj"
      },
      "outputs": [],
      "source": [
        "! python setup.py install > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTmYx-wGNGBV",
        "outputId": "cab6b1cf-7f9b-4a25-cb04-b05729a5c6ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/working\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PQEkGm1XNH4b"
      },
      "outputs": [],
      "source": [
        "rm -rf ./apex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1BD2ql4-NPaE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/working/simclr1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPFxqdFuNRTO",
        "outputId": "95efed5e-64b1-4826-b986-302d8c1bee41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdekun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/working/simclr1/wandb/run-20220727_152913-9mwipb2s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdainty-capybara-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dekun/simclr1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dekun/simclr1/runs/9mwipb2s\u001b[0m\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
            "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
            "# Model Params: 24.62M FLOPs: 1.31G\n",
            "Data loading:   0% 0/97 [00:00<?, ?it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Train Epoch: [1/1] Loss: 6.3196: 100% 97/97 [02:39<00:00,  1.64s/it]\n",
            "Feature extracting: 100% 98/98 [00:31<00:00,  3.15it/s]\n",
            "Test Epoch: [1/1] Acc@1:39.05% Acc@5:88.69%: 100% 20/20 [00:09<00:00,  2.21it/s]\n",
            "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
            "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
            "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
            "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
            "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
            "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss ‚ñà‚ñÑ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  top1 ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  top5 ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss 6.36632\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  top1 39.05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  top5 88.69\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdainty-capybara-1\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/dekun/simclr1/runs/9mwipb2s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220727_152913-9mwipb2s/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python main.py --batch_size 512 --use_checkpoint False --epochs 1 --amp True --amp_level 'O2' --results_path \"results\" --datasets_path \"../../input\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ugKJ0Xa2RdgZ",
        "outputId": "0aef3ad7-aa41-4694-ed82-22a69b62fc95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Files already downloaded and verified\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
            "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "# Model Params: 23.52M FLOPs: 1.31G\n",
            "Train Epoch: [1/1] Loss: 2.3816 ACC@1: 11.42% ACC@5: 54.07%: 100% 49/49 [00:43<00:00,  1.13it/s]\n",
            "Test Epoch: [1/1] Loss: 2.2503 ACC@1: 15.89% ACC@5: 62.88%: 100% 10/10 [00:06<00:00,  1.55it/s]\n"
          ]
        }
      ],
      "source": [
        "!python linear.py --batch_size 1024 --epochs 1 --amp True --amp_level 'O2' --model_path 'results/128_0.5_200_512_1_model.pth' --results_path \"results\" --datasets_path \"../../input\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cloQHzW8Rdga",
        "outputId": "97359c5c-9d42-487d-9f70-2bf4b645fd82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdekun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/working/simclr1/wandb/run-20220727_153411-2gbz96yk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmorning-wildflower-2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dekun/simclr1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dekun/simclr1/runs/2gbz96yk\u001b[0m\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
            "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
            "# Model Params: 24.62M FLOPs: 1.31G\n",
            "Train Epoch: [2/2] Loss: 6.1869: 100% 97/97 [02:39<00:00,  1.65s/it]\n",
            "Feature extracting: 100% 98/98 [00:31<00:00,  3.13it/s]\n",
            "Test Epoch: [2/2] Acc@1:38.66% Acc@5:88.41%: 100% 20/20 [00:09<00:00,  2.10it/s]\n",
            "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
            "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
            "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
            "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
            "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
            "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss ‚ñà‚ñÖ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  top1 ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  top5 ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss 6.21662\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  top1 38.66\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  top5 88.41\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmorning-wildflower-2\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/dekun/simclr1/runs/2gbz96yk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220727_153411-2gbz96yk/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python main.py --resume True --resume_path 'results/128_0.5_200_512_1_model.pth' --batch_size 512 --use_checkpoint False --epochs 2 --amp True --amp_level 'O2' --results_path \"results\" --datasets_path \"../../input\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKMfvl2nNTB_",
        "outputId": "a3676b69-7af6-43a9-e880-cdc8987ed54b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Files already downloaded and verified\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
            "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "# Model Params: 23.52M FLOPs: 1.31G\n",
            "Train Epoch: [1/1] Loss: 2.3898 ACC@1: 11.20% ACC@5: 52.99%: 100% 49/49 [00:43<00:00,  1.13it/s]\n",
            "Test Epoch: [1/1] Loss: 2.2593 ACC@1: 15.45% ACC@5: 61.09%: 100% 10/10 [00:06<00:00,  1.50it/s]\n"
          ]
        }
      ],
      "source": [
        "!python linear.py --batch_size 1024 --epochs 1 --amp True --amp_level 'O2' --model_path 'results/128_0.5_200_512_2_model.pth' --results_path \"results\" --datasets_path \"../../input\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "simclr_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "cc9daa352394ac6fd6b224fdae94757fb224b4aab069bc0189ad8c000c678227"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}