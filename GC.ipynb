{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7039,"status":"ok","timestamp":1658366127988,"user":{"displayName":"K K","userId":"15244586291530659307"},"user_tz":-480},"id":"wvYRyZ27v4NX","outputId":"2e861249-b8c2-45a6-ddfd-ab13fd85e42b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'Kaggle'...\n","remote: Enumerating objects: 3, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (2/2), done.\u001b[K\n","remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (3/3), done.\n","Cloning into 'simclr1'...\n","remote: Enumerating objects: 11, done.\u001b[K\n","remote: Counting objects: 100% (11/11), done.\u001b[K\n","remote: Compressing objects: 100% (11/11), done.\u001b[K\n","remote: Total 11 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (11/11), done.\n","Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/Kaggle/kaggle.json'\n","Downloading cifar-10-batches-py.zip to cifar-10-batches-py\n"," 92% 150M/162M [00:01<00:00, 140MB/s]\n","100% 162M/162M [00:01<00:00, 146MB/s]\n","/content/working\n"]}],"source":["#@title 请输入数据集名称\n","\n","# 导入Kaggle API\n","!git clone https://github.com/JDekun/Kaggle.git\n","import os\n","os.environ['KAGGLE_CONFIG_DIR'] = '/content/Kaggle' #注意kaggle文件夹包含json文件 \n","\n","!mkdir /content/input\n","os.chdir('/content/input')\n","\n","# 数据集名字\n","datasets = [\"janzenliu/cifar-10-batches-py\"]  #@param {type:\"raw\"}\n","len_mydekun = len(datasets)\n","\n","if len_mydekun != 0:\n","  for i in range(len_mydekun):\n","    # 下载数据集\n","    temp = datasets[i]\n","    name, dataset = temp.split('/')\n","    \n","    zip = dataset + '.zip'\n","    !kaggle datasets download -d $temp -p $dataset\n","\n","    # 解压数据集并删除压缩包\n","    !unzip $dataset/$zip -d $dataset > /dev/null 2>&1\n","    !rm -f $dataset/$zip\n","\n","# 设置运行时的根目录\n","!mkdir /content/working\n","os.chdir('/content/working')\n","\n","# GitHub名字\n","github = [\"https://github.com/JDekun/simclr1.git\" ]  #@param {type:\"raw\"}\n","len_github = len(github)\n","if len_github != 0:\n","  for i in range(len_github):\n","    temp = github[i]\n","    !git clone $temp\n","\n","!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KdeadOduWnjJ"},"outputs":[],"source":["!pip install thop\n","!git clone https://github.com/NVIDIA/apex"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":602,"status":"ok","timestamp":1658318550948,"user":{"displayName":"K K","userId":"15244586291530659307"},"user_tz":-480},"id":"acW-qtMsNBLD","outputId":"6beee758-3db3-4c11-b860-6e84ec6c9e12"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/working/apex\n"]}],"source":["cd apex/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eakPCSt_NEFj"},"outputs":[],"source":["! python setup.py install > /dev/null 2>&1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1658318556145,"user":{"displayName":"K K","userId":"15244586291530659307"},"user_tz":-480},"id":"VTmYx-wGNGBV","outputId":"6ed72f31-195c-4175-e62b-5b836117e34c"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/working\n"]}],"source":["cd .."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PQEkGm1XNH4b"},"outputs":[],"source":["rm -rf ./apex"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BD2ql4-NPaE"},"outputs":[],"source":["import os\n","os.chdir('/content/working/simclr1')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1740010,"status":"ok","timestamp":1658320298051,"user":{"displayName":"K K","userId":"15244586291530659307"},"user_tz":-480},"id":"jPFxqdFuNRTO","outputId":"38a011f7-1d79-4408-abcf-ebf87dc76b79"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../cifar-10-python.tar.gz\n","100% 170498071/170498071 [00:05<00:00, 29595160.67it/s]\n","Extracting ../cifar-10-python.tar.gz to ../\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n","\n","Defaults for this optimization level are:\n","enabled                : True\n","opt_level              : O2\n","cast_model_type        : torch.float16\n","patch_torch_functions  : False\n","keep_batchnorm_fp32    : True\n","master_weights         : True\n","loss_scale             : dynamic\n","Processing user overrides (additional kwargs that are not None)...\n","After processing overrides, optimization options are:\n","enabled                : True\n","opt_level              : O2\n","cast_model_type        : torch.float16\n","patch_torch_functions  : False\n","keep_batchnorm_fp32    : True\n","master_weights         : True\n","loss_scale             : dynamic\n","Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n","[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n","[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n","[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n","[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n","[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n","[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n","[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n","# Model Params: 24.62M FLOPs: 1.31G\n","  0% 0/97 [00:00<?, ?it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n","Train Epoch: [1/10] Loss: 6.3104: 100% 97/97 [02:20<00:00,  1.45s/it]\n","Feature extracting: 100% 98/98 [00:23<00:00,  4.25it/s]\n","Test Epoch: [1/10] Acc@1:40.61% Acc@5:90.04%: 100% 20/20 [00:08<00:00,  2.50it/s]\n","Train Epoch: [2/10] Loss: 6.0002: 100% 97/97 [02:20<00:00,  1.45s/it]\n","Feature extracting: 100% 98/98 [00:23<00:00,  4.23it/s]\n","Test Epoch: [2/10] Acc@1:43.68% Acc@5:91.05%: 100% 20/20 [00:07<00:00,  2.51it/s]\n","Train Epoch: [3/10] Loss: 5.8890: 100% 97/97 [02:20<00:00,  1.44s/it]\n","Feature extracting: 100% 98/98 [00:23<00:00,  4.20it/s]\n","Test Epoch: [3/10] Acc@1:46.47% Acc@5:92.06%: 100% 20/20 [00:08<00:00,  2.46it/s]\n","Train Epoch: [4/10] Loss: 5.8409:  36% 35/97 [01:00<01:19,  1.28s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n","Train Epoch: [4/10] Loss: 5.8392: 100% 97/97 [02:20<00:00,  1.45s/it]\n","Feature extracting: 100% 98/98 [00:23<00:00,  4.17it/s]\n","Test Epoch: [4/10] Acc@1:48.71% Acc@5:93.00%: 100% 20/20 [00:07<00:00,  2.53it/s]\n","Train Epoch: [5/10] Loss: 5.7907: 100% 97/97 [02:20<00:00,  1.44s/it]\n","Feature extracting: 100% 98/98 [00:23<00:00,  4.21it/s]\n","Test Epoch: [5/10] Acc@1:50.66% Acc@5:93.44%: 100% 20/20 [00:07<00:00,  2.51it/s]\n","Train Epoch: [6/10] Loss: 5.7344: 100% 97/97 [02:20<00:00,  1.45s/it]\n","Feature extracting: 100% 98/98 [00:23<00:00,  4.21it/s]\n","Test Epoch: [6/10] Acc@1:53.30% Acc@5:94.65%: 100% 20/20 [00:07<00:00,  2.55it/s]\n","Train Epoch: [7/10] Loss: 5.7042: 100% 97/97 [02:19<00:00,  1.44s/it]\n","Feature extracting: 100% 98/98 [00:23<00:00,  4.19it/s]\n","Test Epoch: [7/10] Acc@1:55.01% Acc@5:95.10%: 100% 20/20 [00:08<00:00,  2.49it/s]\n","Train Epoch: [8/10] Loss: 5.6734: 100% 97/97 [02:19<00:00,  1.44s/it]\n","Feature extracting: 100% 98/98 [00:23<00:00,  4.22it/s]\n","Test Epoch: [8/10] Acc@1:57.05% Acc@5:95.73%: 100% 20/20 [00:07<00:00,  2.51it/s]\n","Train Epoch: [9/10] Loss: 5.6512: 100% 97/97 [02:19<00:00,  1.44s/it]\n","Feature extracting: 100% 98/98 [00:23<00:00,  4.19it/s]\n","Test Epoch: [9/10] Acc@1:57.11% Acc@5:96.09%: 100% 20/20 [00:07<00:00,  2.57it/s]\n","Train Epoch: [10/10] Loss: 5.6303: 100% 97/97 [02:19<00:00,  1.44s/it]\n","Feature extracting: 100% 98/98 [00:23<00:00,  4.23it/s]\n","Test Epoch: [10/10] Acc@1:58.39% Acc@5:96.50%: 100% 20/20 [00:07<00:00,  2.56it/s]\n"]}],"source":["!python main.py --batch_size 512 --epochs 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":491079,"status":"ok","timestamp":1658320789123,"user":{"displayName":"K K","userId":"15244586291530659307"},"user_tz":-480},"id":"UKMfvl2nNTB_","outputId":"a0a2e692-c685-47f1-a9ee-576ca24c28f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","Files already downloaded and verified\n","Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n","\n","Defaults for this optimization level are:\n","enabled                : True\n","opt_level              : O2\n","cast_model_type        : torch.float16\n","patch_torch_functions  : False\n","keep_batchnorm_fp32    : True\n","master_weights         : True\n","loss_scale             : dynamic\n","Processing user overrides (additional kwargs that are not None)...\n","After processing overrides, optimization options are:\n","enabled                : True\n","opt_level              : O2\n","cast_model_type        : torch.float16\n","patch_torch_functions  : False\n","keep_batchnorm_fp32    : True\n","master_weights         : True\n","loss_scale             : dynamic\n","Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n","[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n","[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n","[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n","[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n","[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n","[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n","# Model Params: 23.52M FLOPs: 1.31G\n","Train Epoch: [1/10] Loss: 1.5672 ACC@1: 43.49% ACC@5: 88.56%: 100% 49/49 [00:41<00:00,  1.19it/s]\n","Test Epoch: [1/10] Loss: 1.2273 ACC@1: 55.21% ACC@5: 95.44%: 100% 10/10 [00:06<00:00,  1.52it/s]\n","Train Epoch: [2/10] Loss: 1.3803 ACC@1: 50.26% ACC@5: 92.98%: 100% 49/49 [00:40<00:00,  1.21it/s]\n","Test Epoch: [2/10] Loss: 1.2030 ACC@1: 55.74% ACC@5: 95.78%: 100% 10/10 [00:06<00:00,  1.53it/s]\n","Train Epoch: [3/10] Loss: 1.3511 ACC@1: 51.61% ACC@5: 93.28%: 100% 49/49 [00:40<00:00,  1.20it/s]\n","Test Epoch: [3/10] Loss: 1.1609 ACC@1: 57.96% ACC@5: 96.10%: 100% 10/10 [00:06<00:00,  1.54it/s]\n","Train Epoch: [4/10] Loss: 1.3268 ACC@1: 52.34% ACC@5: 93.42%: 100% 49/49 [00:42<00:00,  1.16it/s]\n","Test Epoch: [4/10] Loss: 1.1516 ACC@1: 58.26% ACC@5: 96.24%: 100% 10/10 [00:06<00:00,  1.53it/s]\n","Train Epoch: [5/10] Loss: 1.3269 ACC@1: 52.63% ACC@5: 93.40%: 100% 49/49 [00:41<00:00,  1.18it/s]\n","Test Epoch: [5/10] Loss: 1.1311 ACC@1: 59.18% ACC@5: 96.08%: 100% 10/10 [00:06<00:00,  1.54it/s]\n","Train Epoch: [6/10] Loss: 1.3198 ACC@1: 52.76% ACC@5: 93.59%: 100% 49/49 [00:42<00:00,  1.15it/s]\n","Test Epoch: [6/10] Loss: 1.1134 ACC@1: 59.61% ACC@5: 96.33%: 100% 10/10 [00:06<00:00,  1.50it/s]\n","Train Epoch: [7/10] Loss: 1.3143 ACC@1: 53.37% ACC@5: 93.85%: 100% 49/49 [00:42<00:00,  1.14it/s]\n","Test Epoch: [7/10] Loss: 1.1231 ACC@1: 59.35% ACC@5: 96.20%: 100% 10/10 [00:06<00:00,  1.55it/s]\n","Train Epoch: [8/10] Loss: 1.3020 ACC@1: 53.58% ACC@5: 93.77%: 100% 49/49 [00:42<00:00,  1.16it/s]\n","Test Epoch: [8/10] Loss: 1.1109 ACC@1: 60.08% ACC@5: 96.25%: 100% 10/10 [00:07<00:00,  1.42it/s]\n","Train Epoch: [9/10] Loss: 1.2894 ACC@1: 53.91% ACC@5: 93.94%: 100% 49/49 [00:40<00:00,  1.20it/s]\n","Test Epoch: [9/10] Loss: 1.1096 ACC@1: 59.80% ACC@5: 96.41%: 100% 10/10 [00:06<00:00,  1.50it/s]\n","Train Epoch: [10/10] Loss: 1.3064 ACC@1: 53.55% ACC@5: 93.68%: 100% 49/49 [00:41<00:00,  1.19it/s]\n","Test Epoch: [10/10] Loss: 1.1078 ACC@1: 59.46% ACC@5: 96.38%: 100% 10/10 [00:06<00:00,  1.53it/s]\n"]}],"source":["!python linear.py --batch_size 1024 --epochs 10"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOvp/QyQPsVgt6SmxJve6sw","collapsed_sections":[],"name":"simclr_1.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.12"},"vscode":{"interpreter":{"hash":"cc9daa352394ac6fd6b224fdae94757fb224b4aab069bc0189ad8c000c678227"}}},"nbformat":4,"nbformat_minor":0}
