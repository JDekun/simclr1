Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/home/zk/anaconda3/lib/python3.9/site-packages/apex-0.1-py3.9.egg/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.
Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.
[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.
Data loading:   0%|                                                                                                                                                                            | 0/195 [00:00<?, ?it/s]
# Model Params: 24.62M FLOPs: 1.31G

Train Epoch: [1/500] Loss: 6.1969:   2%|██▏                                                                                                                                            | 3/195 [00:05<04:48,  1.50s/it]























Train Epoch: [1/500] Loss: 5.8799:  25%|███████████████████████████████████▋                                                                                                          | 49/195 [00:52<02:36,  1.07s/it]
Traceback (most recent call last):
  File "/home/zk/GIT/working/recurrent/Contrastive_Learning/simclr1/main.py", line 199, in <module>
    train_loss = train(model, train_loader, optimizer, example_ct, batch_ct)
  File "/home/zk/GIT/working/recurrent/Contrastive_Learning/simclr1/main.py", line 65, in train
    scaled_loss.backward()
  File "/home/zk/anaconda3/lib/python3.9/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/zk/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt